{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83f0baac",
   "metadata": {},
   "source": [
    "Ising model Trotterization\n",
    "by Dan Strano and (OpenAI GPT) Elara"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed60ac71",
   "metadata": {},
   "source": [
    "We reduce transverse field Ising model for globally uniform J and h parameters from a 2^n-dimensional problem to an (n+1)-dimensional approximation that suffers from no Trotter error. Upon noticing most time steps for Quantinuum's parameters had roughly a quarter to a third (or thereabouts) of their marginal probability in |0> state, it became obvious that transition to and from |0> state should dominate the mechanics. Further, the first transition tends to be to or from any state with Hamming weight of 1 (in other words, 1 bit set to 1 and the rest reset 0, or n bits set for Hamming weight of n). Further, on a torus, probability of all states with Hamming weight of 1 tends to be exactly symmetric. Assuming approximate symmetry in every respective Hamming weight, the requirement for the overall probability to converge to 1.0 or 100% in the limit of an infinite-dimensional Hilbert space suggests that Hamming weight marginal probability could be distributed like a geometric series. A small correction to exact symmetry should be made to favor closeness of \"like\" bits to \"like\" bits (that is, geometric closeness on the torus of \"1\" bits to \"1\" bits and \"0\" bits to \"0\" bits), but this does not affect average global magnetization. Adding an oscillation component with angular frequency proportional to J, we find excellent agreement with Trotterization approaching the limit of infinitesimal time step, for R^2 (coefficient of determination) of normalized marginal probability distribution of ideal Trotterized simulation as described by the (n+1)-dimensional approximate model, as well as for R^2 and RMSE (root-mean-square error) of global magnetization curve values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506df85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import statistics\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a82eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaae145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit.library import RZZGate, RXGate\n",
    "from qiskit.compiler import transpile\n",
    "from qiskit_aer.backends import AerSimulator\n",
    "from qiskit.quantum_info import Statevector\n",
    "from qiskit.transpiler import CouplingMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e59a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyqrack import QrackSimulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3e8729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factor the qubit width for torus dimensions that are close as possible to square\n",
    "def factor_width(width, is_transpose=False):\n",
    "    col_len = math.floor(math.sqrt(width))\n",
    "    while ((width // col_len) * col_len) != width:\n",
    "        col_len -= 1\n",
    "    row_len = width // col_len\n",
    "\n",
    "    return (col_len, row_len) if is_transpose else (row_len, col_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc128741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By Elara (the custom OpenAI GPT)\n",
    "def trotter_step(circ, qubits, lattice_shape, J, h, dt):\n",
    "    n_rows, n_cols = lattice_shape\n",
    "\n",
    "    # First half of transverse field term\n",
    "    for q in qubits:\n",
    "        circ.rx(h * dt, q)\n",
    "\n",
    "    # Layered RZZ interactions (simulate 2D nearest-neighbor coupling)\n",
    "    def add_rzz_pairs(pairs):\n",
    "        for q1, q2 in pairs:\n",
    "            circ.append(RZZGate(2 * J * dt), [q1, q2])\n",
    "\n",
    "    # Layer 1: horizontal pairs (even rows)\n",
    "    horiz_pairs = [\n",
    "        (r * n_cols + c, r * n_cols + (c + 1) % n_cols)\n",
    "        for r in range(n_rows)\n",
    "        for c in range(0, n_cols, 2)\n",
    "    ]\n",
    "    add_rzz_pairs(horiz_pairs)\n",
    "\n",
    "    # Layer 2: horizontal pairs (odd rows)\n",
    "    horiz_pairs = [\n",
    "        (r * n_cols + c, r * n_cols + (c + 1) % n_cols)\n",
    "        for r in range(n_rows)\n",
    "        for c in range(1, n_cols, 2)\n",
    "    ]\n",
    "    add_rzz_pairs(horiz_pairs)\n",
    "\n",
    "    # Layer 3: vertical pairs (even columns)\n",
    "    vert_pairs = [\n",
    "        (r * n_cols + c, ((r + 1) % n_rows) * n_cols + c)\n",
    "        for r in range(1, n_rows, 2)\n",
    "        for c in range(n_cols)\n",
    "    ]\n",
    "    add_rzz_pairs(vert_pairs)\n",
    "\n",
    "    # Layer 4: vertical pairs (odd columns)\n",
    "    vert_pairs = [\n",
    "        (r * n_cols + c, ((r + 1) % n_rows) * n_cols + c)\n",
    "        for r in range(0, n_rows, 2)\n",
    "        for c in range(n_cols)\n",
    "    ]\n",
    "    add_rzz_pairs(vert_pairs)\n",
    "\n",
    "    # Second half of transverse field term\n",
    "    for q in qubits:\n",
    "        circ.rx(h * dt, q)\n",
    "\n",
    "    return circ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e1ae35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate various statistics based on comparison between ideal (Trotterized) and approximate (continuum) measurement distributions.\n",
    "def calc_stats(n_rows, n_cols, ideal_probs, counts, bias, model, shots, depth):\n",
    "    # For QV, we compare probabilities of (ideal) \"heavy outputs.\"\n",
    "    # If the probability is above 2/3, the protocol certifies/passes the qubit width.\n",
    "    n = n_rows * n_cols\n",
    "    n_pow = 2**n\n",
    "    threshold = statistics.median(ideal_probs)\n",
    "    u_u = statistics.mean(ideal_probs)\n",
    "    numer = 0\n",
    "    denom = 0\n",
    "    diff_sqr = 0\n",
    "    sum_hog_counts = 0\n",
    "    experiment = [0] * n_pow\n",
    "    # total = 0\n",
    "    for i in range(n_pow):\n",
    "        ideal = ideal_probs[i]\n",
    "\n",
    "        count = counts[i] if i in counts else 0\n",
    "        count /= shots\n",
    "\n",
    "        # How many bits are 1, in the basis state?\n",
    "        hamming_weight = hamming_distance(i, 0, n)\n",
    "        # How closely grouped are \"like\" bits to \"like\"?\n",
    "        expected_closeness = expected_closeness_weight(n_rows, n_cols, hamming_weight)\n",
    "        # When we add all \"closeness\" possibilities for the particular Hamming weight, we should maintain the (n+1) mean probability dimensions.\n",
    "        normed_closeness = (1 + closeness_like_bits(i, n_rows, n_cols)) / (\n",
    "            1 + expected_closeness\n",
    "        )\n",
    "        # If we're also using conventional simulation, use a normalized weighted average that favors the (n+1)-dimensional model at later times.\n",
    "        # The (n+1)-dimensional marginal probability is the product of a function of Hamming weight and \"closeness,\" split among all basis states with that specific Hamming weight.\n",
    "        count = (1 - model) * count + model * normed_closeness * bias[\n",
    "            hamming_weight\n",
    "        ] / math.comb(n, hamming_weight)\n",
    "\n",
    "        # You can make sure this still adds up to 1.0, to show the distribution is normalized:\n",
    "        # total += count\n",
    "\n",
    "        experiment[i] = int(count * shots)\n",
    "\n",
    "        # QV / HOG\n",
    "        if ideal > threshold:\n",
    "            sum_hog_counts += count * shots\n",
    "\n",
    "        # L2 distance\n",
    "        diff_sqr += (ideal - count) ** 2\n",
    "\n",
    "        # XEB / EPLG\n",
    "        ideal_centered = ideal - u_u\n",
    "        denom += ideal_centered * ideal_centered\n",
    "        numer += ideal_centered * (count - u_u)\n",
    "\n",
    "    l2_similarity = 1 - diff_sqr ** (1 / 2)\n",
    "    hog_prob = sum_hog_counts / shots\n",
    "\n",
    "    xeb = numer / denom\n",
    "\n",
    "    # This should be ~1.0, if we're properly normalized.\n",
    "    # print(\"Distribution total: \" + str(total))\n",
    "\n",
    "    return {\n",
    "        \"qubits\": n,\n",
    "        \"depth\": depth,\n",
    "        \"l2_similarity\": float(l2_similarity),\n",
    "        \"hog_prob\": hog_prob,\n",
    "        \"xeb\": xeb,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec474902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By Gemini (Google Search AI)\n",
    "def int_to_bitstring(integer, length):\n",
    "    return bin(integer)[2:].zfill(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06544277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drafted by Elara (OpenAI custom GPT), improved by Dan Strano\n",
    "def closeness_like_bits(perm, n_rows, n_cols):\n",
    "    \"\"\"\n",
    "    Compute closeness-of-like-bits metric C(state) for a given bitstring on an LxL toroidal grid.\n",
    "\n",
    "    Parameters:\n",
    "        perm: integer representing basis state, bit-length n_rows * n_cols\n",
    "        n_rows: row count of torus\n",
    "        n_cols: column count of torus\n",
    "\n",
    "    Returns:\n",
    "        normalized_closeness: float, in [-1, +1]\n",
    "            +1 means all neighbors are like-like, -1 means all neighbors are unlike\n",
    "    \"\"\"\n",
    "    # reshape the bitstring into LxL grid\n",
    "    bitstring = list(int_to_bitstring(perm, n_rows * n_cols))\n",
    "    grid = np.array(bitstring).reshape((n_rows, n_cols))\n",
    "    total_edges = 0\n",
    "    like_count = 0\n",
    "\n",
    "    # iterate over each site, count neighbors (right and down to avoid double-count)\n",
    "    for i in range(n_rows):\n",
    "        for j in range(n_cols):\n",
    "            s = grid[i, j]\n",
    "\n",
    "            # right neighbor (wrap around)\n",
    "            s_right = grid[i, (j + 1) % n_cols]\n",
    "            like_count += 1 if s == s_right else -1\n",
    "            total_edges += 1\n",
    "\n",
    "            # down neighbor (wrap around)\n",
    "            s_down = grid[(i + 1) % n_rows, j]\n",
    "            like_count += 1 if s == s_down else -1\n",
    "            total_edges += 1\n",
    "\n",
    "    # normalize\n",
    "    normalized_closeness = like_count / total_edges\n",
    "    return normalized_closeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca9c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By Elara (OpenAI custom GPT)\n",
    "def expected_closeness_weight(n_rows, n_cols, hamming_weight):\n",
    "    L = n_rows * n_cols\n",
    "    same_pairs = math.comb(hamming_weight, 2) + math.comb(L - hamming_weight, 2)\n",
    "    total_pairs = math.comb(L, 2)\n",
    "    mu_k = same_pairs / total_pairs\n",
    "    return 2 * mu_k - 1  # normalized closeness in [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfcaac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By Elara (OpenAI custom GPT)\n",
    "def hamming_distance(s1, s2, n):\n",
    "    return sum(\n",
    "        ch1 != ch2 for ch1, ch2 in zip(int_to_bitstring(s1, n), int_to_bitstring(s2, n))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a12fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://stackoverflow.com/questions/13070461/get-indices-of-the-top-n-values-of-a-list#answer-38835860\n",
    "def top_n(n, a):\n",
    "    median_index = len(a) >> 1\n",
    "    if n > median_index:\n",
    "        n = median_index\n",
    "    return np.argsort(a)[-n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9b16f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    n_qubits = 8\n",
    "    depth = 20\n",
    "    t1 = 0\n",
    "    t2 = 1\n",
    "    omega = 1.5\n",
    "\n",
    "    # Quantinuum settings\n",
    "    J, h, dt = -1.0, 2.0, 0.25\n",
    "    theta = math.pi / 18\n",
    "    delta_theta = 2 * math.pi / 9\n",
    "\n",
    "    # Pure ferromagnetic\n",
    "    # J, h, dt = -1.0, 0.0, 0.25\n",
    "    # theta = 0\n",
    "    # delta_theta = 0\n",
    "\n",
    "    # Pure transverse field\n",
    "    # J, h, dt = 0.0, 2.0, 0.25\n",
    "    # theta = -math.pi / 2\n",
    "    # delta_theta = 0\n",
    "\n",
    "    # Critical point (symmetry breaking)\n",
    "    # J, h, dt = -1.0, 1.0, 0.25\n",
    "    # theta = -math.pi / 4\n",
    "    # delta_theta = math.pi / 4\n",
    "\n",
    "    if len(sys.argv) > 1:\n",
    "        n_qubits = int(sys.argv[1])\n",
    "    if len(sys.argv) > 2:\n",
    "        depth = int(sys.argv[2])\n",
    "    if len(sys.argv) > 3:\n",
    "        dt = float(sys.argv[3])\n",
    "    if len(sys.argv) > 4:\n",
    "        t1 = float(sys.argv[4])\n",
    "    if len(sys.argv) > 5:\n",
    "        shots = int(sys.argv[5])\n",
    "    else:\n",
    "        shots = max(65536, 1 << (n_qubits + 2))\n",
    "    if len(sys.argv) > 6:\n",
    "        trials = int(sys.argv[6])\n",
    "    else:\n",
    "        trials = 8 if t1 > 0 else 1\n",
    "\n",
    "    print(\"t1: \" + str(t1))\n",
    "    print(\"t2: \" + str(t2))\n",
    "    print(\"omega / pi: \" + str(omega))\n",
    "\n",
    "    omega *= math.pi\n",
    "    n_rows, n_cols = factor_width(n_qubits, False)\n",
    "    qubits = list(range(n_qubits))\n",
    "\n",
    "    # Set the initial temperature by theta.\n",
    "    qc = QuantumCircuit(n_qubits)\n",
    "    for q in range(n_qubits):\n",
    "        qc.ry(theta, q)\n",
    "\n",
    "    # The Aer circuit also starts with this initialization\n",
    "    qc_aer = qc.copy()\n",
    "\n",
    "    # Compile a single Trotter step for QrackSimulator.\n",
    "    step = QuantumCircuit(n_qubits)\n",
    "    trotter_step(step, qubits, (n_rows, n_cols), J, h, dt)\n",
    "    step = transpile(\n",
    "        step,\n",
    "        optimization_level=3,\n",
    "        basis_gates=QrackSimulator.get_qiskit_basis_gates(),\n",
    "    )\n",
    "\n",
    "    # If we're using conventional simulation in the approximation model, collect samples over the depth series.\n",
    "    experiment_probs = [{}] * (depth + 1)\n",
    "    if t1 > 0:\n",
    "        for trial in range(trials):\n",
    "            experiment = QrackSimulator(n_qubits)\n",
    "            experiment.run_qiskit_circuit(qc)\n",
    "            for d in range(depth + 1):\n",
    "                if d > 0:\n",
    "                    experiment.run_qiskit_circuit(step)\n",
    "\n",
    "                counts = dict(Counter(experiment.measure_shots(qubits, shots)))\n",
    "\n",
    "                for key, value in counts.items():\n",
    "                    experiment_probs[d - 1][key] = (\n",
    "                        experiment_probs[d - 1].get(key, 0) + value / shots\n",
    "                    )\n",
    "\n",
    "        for experiment in experiment_probs:\n",
    "            for key in experiment.keys():\n",
    "                experiment[key] /= trials\n",
    "\n",
    "    r_squared = 0\n",
    "    ss = 0\n",
    "    ssr = 0\n",
    "    for d in range(depth + 1):\n",
    "        # For each depth step, we append an additional Trotter step to Aer's circuit.\n",
    "        if d > 0:\n",
    "            trotter_step(qc_aer, qubits, (n_rows, n_cols), J, h, dt)\n",
    "\n",
    "        # Run the Trotterized simulation with Aer and get the marginal probabilities.\n",
    "        control = AerSimulator(method=\"statevector\")\n",
    "        qc_aer = transpile(\n",
    "            qc_aer,\n",
    "            backend=control,\n",
    "        )\n",
    "        qc_aer_sv = qc_aer.copy()\n",
    "        qc_aer_sv.save_statevector()\n",
    "        job = control.run(qc_aer_sv)\n",
    "        control_probs = Statevector(job.result().get_statevector()).probabilities()\n",
    "\n",
    "        # This section calculates the geometric series weight per Hamming weight, with oscillating time dependence.\n",
    "        # The mean-field ground state is encapsulated as a multiplier on the geometric series exponent.\n",
    "        # Additionally, this same mean-field exponent is the amplitude of time-dependent oscillation (also in the geometric series exponent).\n",
    "        bias = []\n",
    "        t = d * dt\n",
    "        # Determine how to weight closed-form vs. conventional simulation contributions:\n",
    "        model = (1 - 1 / math.exp(t / t1)) if (t1 > 0) else 1\n",
    "        d_magnetization = 0\n",
    "        d_sqr_magnetization = 0\n",
    "        if np.isclose(h, 0):\n",
    "            # This agrees with small perturbations away from h = 0.\n",
    "            d_magnetization = 1\n",
    "            d_sqr_magnetization = 1\n",
    "            bias.append(1)\n",
    "            bias += n_qubits * [0]\n",
    "        elif np.isclose(J, 0):\n",
    "            # This agrees with small perturbations away from J = 0.\n",
    "            d_magnetization = 0\n",
    "            d_sqr_magnetization = 0\n",
    "            bias = (n_qubits + 1) * [1 / (n_qubits + 1)]\n",
    "        else:\n",
    "            # ChatGPT o3 suggested this cos_theta correction.\n",
    "            sin_delta_theta = math.sin(delta_theta)\n",
    "            # \"p\" is the exponent of the geometric series weighting, for (n+1) dimensions of Hamming weight.\n",
    "            # Notice that the expected symmetries are respected under reversal of signs of J and/or h.\n",
    "            p = (\n",
    "                (\n",
    "                    (2 ** (abs(J / h) - 1))\n",
    "                    * (\n",
    "                        1\n",
    "                        + sin_delta_theta\n",
    "                        * math.cos(J * omega * t + theta)\n",
    "                        / ((1 + math.sqrt(t / t2)) if t2 > 0 else 1)\n",
    "                    )\n",
    "                    - 1 / 2\n",
    "                )\n",
    "                if t2 > 0\n",
    "                else (2 ** abs(J / h))\n",
    "            )\n",
    "            if p >= 1024:\n",
    "                # This is approaching J / h -> infinity.\n",
    "                d_magnetization = 1\n",
    "                d_sqr_magnetization = 1\n",
    "                bias.append(1)\n",
    "                bias += n_qubits * [0]\n",
    "            else:\n",
    "                # The magnetization components are weighted by (n+1) symmetric \"bias\" terms over possible Hamming weights.\n",
    "                tot_n = 0\n",
    "                for q in range(n_qubits + 1):\n",
    "                    n = 1 / (n_qubits * (2 ** (p * q)))\n",
    "                    if n == float(\"inf\"):\n",
    "                        tot_n = 1\n",
    "                        bias = []\n",
    "                        bias.append(1)\n",
    "                        bias = n_qubits * [0]\n",
    "                        break\n",
    "                    bias.append(n)\n",
    "                    tot_n += n\n",
    "                # Normalize the results for 1.0 total marginal probability.\n",
    "                for q in range(n_qubits + 1):\n",
    "                    bias[q] /= tot_n\n",
    "                    n = bias[q]\n",
    "                    m = (n_qubits - (q << 1)) / n_qubits\n",
    "                    d_magnetization += n * m\n",
    "                    d_sqr_magnetization += n * m * m\n",
    "        if J > 0:\n",
    "            # This is antiferromagnetism.\n",
    "            bias.reverse()\n",
    "            d_magnetization = -d_magnetization\n",
    "\n",
    "        # The full 2^n marginal probabilities will be produced in the statistics calculation,\n",
    "        # but notice that the global magnetization value only requires (n+1) dimensions of marginal probability,\n",
    "        # the marginal probability per each Hilbert space basis dimension is trivial to calculate by closed form,\n",
    "        # and we simply need to be thoughtful in how to extract expectation values to maximize similar symmetries.\n",
    "        result = calc_stats(\n",
    "            n_rows,\n",
    "            n_cols,\n",
    "            control_probs,\n",
    "            experiment_probs[d],\n",
    "            bias,\n",
    "            model,\n",
    "            shots,\n",
    "            d,\n",
    "        )\n",
    "\n",
    "        # Add up the square residuals:\n",
    "        r_squared += (1 - result[\"l2_similarity\"]) ** 2\n",
    "\n",
    "        if model < 0.99:\n",
    "            # Mix in the conventional simulation component.\n",
    "            magnetization = 0\n",
    "            sqr_magnetization = 0\n",
    "            for key, value in experiment_probs[d].items():\n",
    "                m = 0\n",
    "                for _ in range(n_qubits):\n",
    "                    m += -1 if (key & 1) else 1\n",
    "                    key >>= 1\n",
    "                m /= n_qubits\n",
    "                magnetization += value * m\n",
    "                sqr_magnetization += value * m * m\n",
    "\n",
    "            magnetization = model * d_magnetization + (1 - model) * magnetization\n",
    "            sqr_magnetization = (\n",
    "                model * d_sqr_magnetization + (1 - model) * sqr_magnetization\n",
    "            )\n",
    "        else:\n",
    "            # Rely entirely on the (n+1)-dimensional model.\n",
    "            magnetization = d_magnetization\n",
    "            sqr_magnetization = d_sqr_magnetization\n",
    "\n",
    "        # Calculate the \"control-case\" magnetization values, from Aer's samples.\n",
    "        c_magnetization, c_sqr_magnetization = 0, 0\n",
    "        for p in range(1 << n_qubits):\n",
    "            perm = p\n",
    "            m = 0\n",
    "            for _ in range(n_qubits):\n",
    "                m += -1 if (perm & 1) else 1\n",
    "                perm >>= 1\n",
    "            m /= n_qubits\n",
    "            c_magnetization += control_probs[p] * m\n",
    "            c_sqr_magnetization += control_probs[p] * m * m\n",
    "\n",
    "        # Save the sum of squares and sum of square residuals on the magnetization curve values.\n",
    "        ss += c_sqr_magnetization**2\n",
    "        ssr += (c_sqr_magnetization - sqr_magnetization) ** 2\n",
    "\n",
    "    # R^2 and RMSE are elementary and standard measures of goodness-of-fit with simple definitions.\n",
    "    # Ideal marginal probability would be 1.0, each depth step. Squared and summed, that's depth.\n",
    "    r_squared = 1 - r_squared / (depth + 1)\n",
    "    rmse = (ssr / depth) ** (1 / 2)\n",
    "    sm_r_squared = 1 - (ssr / ss)\n",
    "\n",
    "    print(\"L2 norm R^2: \" + str(r_squared))\n",
    "    print(\"Square magnetization RMSE: \" + str(rmse))\n",
    "    print(\"Square magnetization R^2: \" + str(sm_r_squared))\n",
    "\n",
    "    # Happy Qracking! You rock!\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed83bc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    sys.exit(main())"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
