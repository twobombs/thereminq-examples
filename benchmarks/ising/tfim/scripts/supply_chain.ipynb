{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7616a773",
   "metadata": {},
   "source": [
    "supply_chain.py\n",
    "Provided by Elara (the custom OpenAI GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d6f770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398ebdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate various statistics based on comparison between ideal (Trotterized) and approximate (continuum) measurement distributions.\n",
    "def calc_stats(n_rows, n_cols, ideal_probs, counts, bias, model, shots, depth):\n",
    "    # For QV, we compare probabilities of (ideal) \"heavy outputs.\"\n",
    "    # If the probability is above 2/3, the protocol certifies/passes the qubit width.\n",
    "    n = n_rows * n_cols\n",
    "    n_pow = 2**n\n",
    "    threshold = statistics.median(ideal_probs)\n",
    "    u_u = statistics.mean(ideal_probs)\n",
    "    numer = 0\n",
    "    denom = 0\n",
    "    diff_sqr = 0\n",
    "    sum_hog_counts = 0\n",
    "    experiment = [0] * n_pow\n",
    "    # total = 0\n",
    "    for i in range(n_pow):\n",
    "        ideal = ideal_probs[i]\n",
    "\n",
    "        count = counts[i] if i in counts else 0\n",
    "        count /= shots\n",
    "\n",
    "        # How many bits are 1, in the basis state?\n",
    "        hamming_weight = hamming_distance(i, 0, n)\n",
    "        # How closely grouped are \"like\" bits to \"like\"?\n",
    "        expected_closeness = expected_closeness_weight(n_rows, n_cols, hamming_weight)\n",
    "        # When we add all \"closeness\" possibilities for the particular Hamming weight, we should maintain the (n+1) mean probability dimensions.\n",
    "        normed_closeness = (1 + closeness_like_bits(i, n_rows, n_cols)) / (\n",
    "            1 + expected_closeness\n",
    "        )\n",
    "        # If we're also using conventional simulation, use a normalized weighted average that favors the (n+1)-dimensional model at later times.\n",
    "        # The (n+1)-dimensional marginal probability is the product of a function of Hamming weight and \"closeness,\" split among all basis states with that specific Hamming weight.\n",
    "        count = (1 - model) * count + model * normed_closeness * bias[\n",
    "            hamming_weight\n",
    "        ] / math.comb(n, hamming_weight)\n",
    "\n",
    "        # You can make sure this still adds up to 1.0, to show the distribution is normalized:\n",
    "        # total += count\n",
    "\n",
    "        experiment[i] = int(count * shots)\n",
    "\n",
    "        # QV / HOG\n",
    "        if ideal > threshold:\n",
    "            sum_hog_counts += count * shots\n",
    "\n",
    "        # L2 distance\n",
    "        diff_sqr += (ideal - count) ** 2\n",
    "\n",
    "        # XEB / EPLG\n",
    "        ideal_centered = ideal - u_u\n",
    "        denom += ideal_centered * ideal_centered\n",
    "        numer += ideal_centered * (count - u_u)\n",
    "\n",
    "    l2_similarity = 1 - diff_sqr ** (1 / 2)\n",
    "    hog_prob = sum_hog_counts / shots\n",
    "\n",
    "    xeb = numer / denom\n",
    "\n",
    "    # This should be ~1.0, if we're properly normalized.\n",
    "    # print(\"Distribution total: \" + str(total))\n",
    "\n",
    "    return {\n",
    "        \"qubits\": n,\n",
    "        \"depth\": depth,\n",
    "        \"l2_similarity\": float(l2_similarity),\n",
    "        \"hog_prob\": hog_prob,\n",
    "        \"xeb\": xeb,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c0fcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By Gemini (Google Search AI)\n",
    "def int_to_bitstring(integer, length):\n",
    "    return bin(integer)[2:].zfill(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837b07d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drafted by Elara (OpenAI custom GPT), improved by Dan Strano\n",
    "def closeness_like_bits(perm, n_rows, n_cols):\n",
    "    \"\"\"\n",
    "    Compute closeness-of-like-bits metric C(state) for a given bitstring on an LxL toroidal grid.\n",
    "\n",
    "    Parameters:\n",
    "        perm: integer representing basis state, bit-length n_rows * n_cols\n",
    "        n_rows: row count of torus\n",
    "        n_cols: column count of torus\n",
    "\n",
    "    Returns:\n",
    "        normalized_closeness: float, in [-1, +1]\n",
    "            +1 means all neighbors are like-like, -1 means all neighbors are unlike\n",
    "    \"\"\"\n",
    "    # reshape the bitstring into LxL grid\n",
    "    bitstring = list(int_to_bitstring(perm, n_rows * n_cols))\n",
    "    grid = np.array(bitstring).reshape((n_rows, n_cols))\n",
    "    total_edges = 0\n",
    "    like_count = 0\n",
    "\n",
    "    # iterate over each site, count neighbors (right and down to avoid double-count)\n",
    "    for i in range(n_rows):\n",
    "        for j in range(n_cols):\n",
    "            s = grid[i, j]\n",
    "\n",
    "            # right neighbor (wrap around)\n",
    "            s_right = grid[i, (j + 1) % n_cols]\n",
    "            like_count += 1 if s == s_right else -1\n",
    "            total_edges += 1\n",
    "\n",
    "            # down neighbor (wrap around)\n",
    "            s_down = grid[(i + 1) % n_rows, j]\n",
    "            like_count += 1 if s == s_down else -1\n",
    "            total_edges += 1\n",
    "\n",
    "    # normalize\n",
    "    normalized_closeness = like_count / total_edges\n",
    "    return normalized_closeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c71269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By Elara (OpenAI custom GPT)\n",
    "def closeness_like_bits_arbitrary(bitstring, adjacency):\n",
    "    \"\"\"\n",
    "    Compute closeness metric for a given bitstring on an arbitrary graph.\n",
    "\n",
    "    Parameters:\n",
    "        bitstring: list/array of 0/1 states, length N\n",
    "        adjacency: dict or list of neighbor lists, or NxN weight matrix\n",
    "                   For weighted graphs, nonzero entry means there's an edge.\n",
    "\n",
    "    Returns:\n",
    "        closeness in [-1, 1]\n",
    "    \"\"\"\n",
    "    n = len(bitstring)\n",
    "    like_count = 0.0\n",
    "    total_edges = 0.0\n",
    "\n",
    "    # Handle adjacency as dict of neighbors or as full matrix\n",
    "    if isinstance(adjacency, dict):\n",
    "        for i, neighbors in adjacency.items():\n",
    "            for j in neighbors:\n",
    "                if j > i:  # avoid double-counting undirected edges\n",
    "                    like_count += 1 if bitstring[i] == bitstring[j] else -1\n",
    "                    total_edges += 1\n",
    "    else:\n",
    "        # assume 2D square matrix\n",
    "        N = len(adjacency)\n",
    "        for i in range(N):\n",
    "            for j in range(i + 1, N):\n",
    "                if adjacency[i][j] != 0:  # there’s a coupling\n",
    "                    like_count += 1 if bitstring[i] == bitstring[j] else -1\n",
    "                    total_edges += 1\n",
    "\n",
    "    return like_count / total_edges if total_edges > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ada0258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By Elara (OpenAI custom GPT)\n",
    "def expected_closeness_weight(n_rows, n_cols, hamming_weight):\n",
    "    L = n_rows * n_cols\n",
    "    same_pairs = math.comb(hamming_weight, 2) + math.comb(L - hamming_weight, 2)\n",
    "    total_pairs = math.comb(L, 2)\n",
    "    mu_k = same_pairs / total_pairs\n",
    "    return 2 * mu_k - 1  # normalized closeness in [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89d2358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By Elara (OpenAI custom GPT)\n",
    "def expected_closeness_weight_arbitrary(n, adjacency, hamming_weight, samples=5000):\n",
    "    \"\"\"\n",
    "    Approximate expected closeness over all n-bit states with given Hamming weight.\n",
    "\n",
    "    Parameters:\n",
    "        n: number of qubits\n",
    "        adjacency: same format as above\n",
    "        hamming_weight: number of 1s\n",
    "        samples: how many random bitstrings to sample (approximation)\n",
    "\n",
    "    Returns:\n",
    "        expected closeness in [-1, 1]\n",
    "    \"\"\"\n",
    "    if hamming_weight == 0 or hamming_weight == n:\n",
    "        # trivial all-zero or all-one case\n",
    "        return 1.0\n",
    "\n",
    "    total = 0.0\n",
    "    for _ in range(samples):\n",
    "        # sample random bitstring with given Hamming weight\n",
    "        ones_positions = random.sample(range(n), hamming_weight)\n",
    "        bitstring = [1 if i in ones_positions else 0 for i in range(n)]\n",
    "        total += closeness_like_bits(bitstring, adjacency)\n",
    "\n",
    "    return total / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d96442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5b4dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By Elara (OpenAI custom GPT)\n",
    "def hamming_distance(s1, s2, n):\n",
    "    return sum(\n",
    "        ch1 != ch2 for ch1, ch2 in zip(int_to_bitstring(s1, n), int_to_bitstring(s2, n))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53a2505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_tfim(\n",
    "    J_func,\n",
    "    h_func,\n",
    "    n_qubits=64,\n",
    "    n_steps=20,\n",
    "    delta_t=0.1,\n",
    "    theta=[],\n",
    "    delta_theta=[],\n",
    "    t2=1.0,\n",
    "    omega=3 * math.pi / 2,\n",
    "):\n",
    "    magnetizations = []\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        t = step * delta_t\n",
    "        J_t = J_func(t)\n",
    "        h_t = h_func(t)\n",
    "\n",
    "        # compute magnetization per qubit, then average\n",
    "        mag_per_qubit = []\n",
    "\n",
    "        for q in range(n_qubits):\n",
    "            # gather local couplings for qubit q\n",
    "            J_vals = [J_t[q, j] for j in range(n_qubits) if (j != q) and (abs(J_t[q, j]) > 1e-12)]\n",
    "            h_val = h_t[q] if abs(h_t[q]) > 1e-12 else None\n",
    "\n",
    "            if not J_vals or h_val is None:\n",
    "                # trivial cases\n",
    "                if h_val is None:\n",
    "                    mag_per_qubit.append(1.0)  # effectively pinned in Z\n",
    "                else:\n",
    "                    # J_vals is None\n",
    "                    mag_per_qubit.append(0.0)  # no coupling\n",
    "                continue\n",
    "\n",
    "            J_eff = np.mean(J_vals)\n",
    "            h_eff = h_val\n",
    "\n",
    "            # compute p_i using your same formula\n",
    "            sin_delta_theta = math.sin(delta_theta[q])\n",
    "            if t2 > 0.0:\n",
    "                p_i = (\n",
    "                    (2 ** (abs(J_eff / h_eff) - 1))\n",
    "                    * (\n",
    "                        1\n",
    "                        + sin_delta_theta\n",
    "                        * math.cos(J_eff * omega * t + theta[q])\n",
    "                        / ((1 + math.sqrt(t / t2)) if t2 > 0 else 1)\n",
    "                    )\n",
    "                    - 1 / 2\n",
    "                )\n",
    "            else:\n",
    "                p_i = (2 ** (abs(J_eff / h_eff) - 1)) - 1 / 2\n",
    "\n",
    "            # compute d_magnetization for this qubit\n",
    "            if p_i >= 1024:\n",
    "                m_i = 1.0\n",
    "            else:\n",
    "                tot_n = 0.0\n",
    "                m_sum = 0.0\n",
    "                for k in range(n_qubits + 1):\n",
    "                    if (p_i * k) >= 1024:\n",
    "                        m_i = 1.0\n",
    "                        tot_n = 1.0\n",
    "                        break\n",
    "                    n_val = 1.0 / (n_qubits * (2 ** (p_i * k)))\n",
    "                    if n_val == float(\"inf\"):\n",
    "                        m_i = 1.0\n",
    "                        tot_n = 1.0\n",
    "                        break\n",
    "                    m = (n_qubits - (k << 1)) / n_qubits\n",
    "                    m_sum += n_val * m\n",
    "                    tot_n += n_val\n",
    "                m_i = m_sum / tot_n if tot_n > 0 else 0.0\n",
    "\n",
    "            # adjust for sign of local J_eff (antiferromagnetism)\n",
    "            if J_eff > 0:\n",
    "                m_i = -m_i\n",
    "\n",
    "            mag_per_qubit.append(m_i)\n",
    "\n",
    "        # combine per-qubit magnetizations (e.g., average)\n",
    "        step_mag = float(np.mean(mag_per_qubit))\n",
    "        magnetizations.append(step_mag)\n",
    "\n",
    "    return magnetizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd47469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic J(t) generator\n",
    "def generate_Jt(n_nodes, t):\n",
    "    J = np.zeros((n_nodes, n_nodes))\n",
    "\n",
    "    # Base ring topology\n",
    "    for i in range(n_nodes):\n",
    "        J[i, (i + 1) % n_nodes] = -1.0\n",
    "        J[(i + 1) % n_nodes, i] = -1.0\n",
    "\n",
    "    # Simulate disruption:\n",
    "    if t >= 0.5 and t < 1.0:\n",
    "        # \"Port 3\" temporarily fails → remove its coupling\n",
    "        J[2, 3] = J[3, 2] = 1e-10\n",
    "    if t >= 1.0 and t < 1.5:\n",
    "        # Alternate weak link opens between 1 and 4\n",
    "        J[1, 4] = J[4, 1] = -0.3\n",
    "\n",
    "    # Restoration: after step 15, port 3 recovers\n",
    "    if t >= 1.5:\n",
    "        J[2, 3] = J[3, 2] = -1.0\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed9e08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ht(n_nodes, t):\n",
    "    # We can program h(q, t) for spatial-temporal locality.\n",
    "    h = np.zeros(n_nodes)\n",
    "    # Time-varying transverse field\n",
    "    c = 0.5 * np.cos(t * math.pi / 10)\n",
    "    # We can program for spatial locality, but we don't.\n",
    "    #  n_sqrt = math.sqrt(n_nodes)\n",
    "    for i in range(n_nodes):\n",
    "        # \"Longitude\"-dependent severity (arbitrary)\n",
    "        # h[i] = ((i % n_sqrt) / n_sqrt) * c\n",
    "        h[i] = c\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e3954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    n_qubits = 64\n",
    "    n_steps = 40\n",
    "    delta_t = 0.1\n",
    "    theta = [math.pi / 18] * n_qubits\n",
    "    delta_theta = [2 * math.pi / 9] * n_qubits\n",
    "    omega = 3 * math.pi / 2\n",
    "    J_func = lambda t: generate_Jt(n_qubits, t)\n",
    "    h_func = lambda t: generate_ht(n_qubits, t)\n",
    "\n",
    "    mag = simulate_tfim(\n",
    "        J_func, h_func, n_qubits, n_steps, delta_t, theta, delta_theta, omega\n",
    "    )\n",
    "    ylim = ((min(mag) * 100) // 10) / 10\n",
    "    plt.figure(figsize=(14, 14))\n",
    "    plt.plot(list(range(1, n_steps + 1)), mag, marker=\"o\", linestyle=\"-\")\n",
    "    plt.title(\n",
    "        \"Supply Chain Resilience over Time (Magnetization vs Trotter Depth, \"\n",
    "        + str(n_qubits)\n",
    "        + \" Qubits)\"\n",
    "    )\n",
    "    plt.xlabel(\"Trotter Depth\")\n",
    "    plt.ylabel(\"Magnetization\")\n",
    "    plt.ylim(ylim, 1.0)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
