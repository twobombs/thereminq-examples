#!/usr/bin/env python3
# graph for the output of otoc_validationisingonly.sh
# generated by gemini25
# custom made for the output of a 512x512 run
# 

import re
import ast
import pandas as pd
from vedo import Plotter, settings
from vedo import Grid, Points # Import Grid and Points
import numpy as np
from scipy.interpolate import griddata
from pathlib import Path
import concurrent.futures
import os

LOG_DIR = "otoc_sweep_log"
OUTPUT_FILE = "otoc_sweep_3d_plot.png"

# --- HANG FIXES ---
DICT_SEARCH_LINE_LIMIT = 20000    # Max lines to search *inside* a dict
GLOBAL_SEARCH_LINE_LIMIT = 100000 # Max lines to read from one file
# --- END HANG FIXES ---

def parse_log_file(file_path):
    match = re.search(r"q(\d+)_d(\d+)\.log", file_path.name)
    if not match:
        return None
    
    # --- Return flags for all possible errors ---
    result = {
        "qubits": int(match.group(1)),
        "depth": int(match.group(2)),
        "time": None,
        "prob_zero": None, # Default to None, not 0.0
        "key_0_missing": False,
        "header_missing": False,
        "time_missing": False,
        "time_parse_error": False,
        "time_non_positive": False,
        "eof_in_dict": False,
        "global_line_limit_exceeded": False
    }
    # --- End flags ---

    time_match = None
    prob_zero_val = None
    found_header = False  
    in_dict = False      
    brace_level = 0
    prob_regex = re.compile(r"\b0:\s*([\d\.eE+-]+)")
    
    dict_line_counter = 0
    total_line_counter = 0

    try:
        with file_path.open('r') as f:
            for line in f:
                # --- HANG FIX 2: Global line limit ---
                total_line_counter += 1
                if total_line_counter > GLOBAL_SEARCH_LINE_LIMIT:
                    result["global_line_limit_exceeded"] = True
                    print(f"\n--- PARSER WARNING: File {file_path.name} exceeded GLOBAL search limit {GLOBAL_SEARCH_LINE_LIMIT}. Skipping. ---")
                    return result  
                # --- END HANG FIX 2 ---

                time_search = re.search(r"real\s+(\d+)m([\d\.]+)s", line)
                if time_search:
                    time_match = time_search
                
                if not found_header:
                    if "PyQrackIsing Probabilities:" in line:
                        found_header = True
                        start_match = re.search(r"(\{)", line)
                        if start_match:
                            in_dict = True
                            brace_level += line.count('{') - line.count('}')
                            if prob_zero_val is None:
                                prob_match = prob_regex.search(line)
                                if prob_match:
                                    prob_zero_val = float(prob_match.group(1))
                
                elif found_header and not in_dict:
                    start_match = re.search(r"(\{)", line)
                    if start_match:
                        in_dict = True
                        brace_level += line.count('{') - line.count('}')
                        if prob_zero_val is None:
                            prob_match = prob_regex.search(line)
                            if prob_match:
                                prob_zero_val = float(prob_match.group(1))
                
                elif in_dict:
                    # --- HANG FIX 1: Dict line limit ---
                    dict_line_counter += 1
                    if dict_line_counter > DICT_SEARCH_LINE_LIMIT:
                        print(f"\n--- PARSER WARNING: File {file_path.name} exceeded DICT search limit {DICT_SEARCH_LINE_LIMIT}. Skipping dict. ---")
                        in_dict = False
                        brace_level = 0
                        continue
                    # --- END HANG FIX 1 ---
                
                    if prob_zero_val is None:
                        prob_match = prob_regex.search(line)
                        if prob_match:
                            prob_zero_val = float(prob_match.group(1))
                    brace_level += line.count('{') - line.count('}')
                    if brace_level == 0:
                        in_dict = False
                        
        if in_dict:
            result["eof_in_dict"] = True

        # --- Parse Time ---
        if not time_match:
            result["time_missing"] = True
            return result  
        try:
            minutes = float(time_match.group(1))
            seconds = float(time_match.group(2))
            total_seconds = (minutes * 60) + seconds
            if total_seconds <= 0:
                result["time_non_positive"] = True
                return result  
            result["time"] = total_seconds
        except (ValueError, TypeError) as e:
            result["time_parse_error"] = True
            return result  

        # --- Parse Probs ---
        if not found_header:
            result["header_missing"] = True
        elif prob_zero_val is None:
            result["key_0_missing"] = True
        else:
            result["prob_zero"] = prob_zero_val

        return result
        
    except Exception as e:
        return {"parse_exception": True, "file_name": file_path.name, "error": str(e)}


def main():
    log_path = Path(LOG_DIR)
    if not log_path.is_dir():
        print(f"Error: Log directory '{LOG_DIR}' not found.")
        return

    print(f"Scanning {LOG_DIR} for log files...")
    log_files = list(log_path.glob("q*_d*.log"))
    if not log_files:
        print(f"Error: No log files found in '{LOG_DIR}'.")
        return

    num_workers = os.cpu_count()
    print(f"Parsing {len(log_files)} log files using up to {num_workers} processes...")
    
    results_list = []
    
    # --- Counters for all warnings ---
    warnings = {
        "key_0_missing": 0,
        "header_missing": 0,
        "time_missing": 0,
        "time_parse_error": 0,
        "time_non_positive": 0,
        "eof_in_dict": 0,
        "global_line_limit_exceeded": 0
    }
    parse_exceptions = 0

    # --- HANG FIX: Use executor.map for lazy iteration ---
    with concurrent.futures.ProcessPoolExecutor(max_workers=num_workers) as executor:
        results_iterator = executor.map(parse_log_file, log_files)
        
        print(f"Parsing {len(log_files)} files. Printing dot every 100 files:")
        processed_count = 0
        
        for result in results_iterator:
            processed_count += 1
            if processed_count % 100 == 0:
                print(".", end="", flush=True)  
            
            if result is not None:
                if result.get("parse_exception", False):
                    parse_exceptions += 1
                    continue
                
                results_list.append(result)
                for key in warnings:
                    if result.get(key, False):
                        warnings[key] += 1
    # --- END HANG FIX ---
        
    print(f"\nParsing complete.")  
        
    print(f"\nSuccessfully parsed {len(results_list)} log files.")
    
    # --- Print all warning summaries ---
    if warnings["header_missing"] > 0:
        print(f"INFO: {warnings['header_missing']} log files were missing the 'PyQrackIsing Probabilities' header.")
    if warnings["key_0_missing"] > 0:
        print(f"INFO: {warnings['key_0_missing']} log files had the header but were missing the '0' key.")
    if warnings["time_missing"] > 0:
        print(f"WARNING: {warnings['time_missing']} log files were missing the 'real time' string.")
    if warnings["time_parse_error"] > 0:
        print(f"WARNING: {warnings['time_parse_error']} log files had an unparseable 'real time' string.")
    if warnings["time_non_positive"] > 0:
        print(f"WARNING: {warnings['time_non_positive']} log files had a non-positive 'real time'.")
    if warnings["eof_in_dict"] > 0:
        print(f"WARNING: {warnings['eof_in_dict']} log files ended unexpectedly while inside a dict.")
    if warnings["global_line_limit_exceeded"] > 0:
        print(f"WARNING: {warnings['global_line_limit_exceeded']} log files exceeded the global line limit and were skipped.")
    if parse_exceptions > 0:
        print(f"WARNING: {parse_exceptions} files caused a critical parse exception.")
    
    df = pd.DataFrame(results_list)

    # --- Data Cleaning ---
    original_count = len(df)
    # Drop rows where 'prob_zero' is missing (was None) or time is missing
    df = df.dropna(subset=['time', 'prob_zero'])  
    
    # --- FIX 12: Ensure no zero or negative probabilities before log ---
    # We drop 'prob_zero' == 0.0 because log(0) is -inf
    # We keep values > 0
    valid_prob_mask = df['prob_zero'] > 0
    invalid_prob_count = len(df) - valid_prob_mask.sum()
    if invalid_prob_count > 0:
        print(f"INFO: Dropped {invalid_prob_count} rows where 'prob_zero' was 0.0 (incompatible with log scale).")
        df = df[valid_prob_mask]
        
    cleaned_count = len(df)
    if original_count != cleaned_count:
        print(f"INFO: Total dropped rows (missing data or prob_zero=0): {original_count - cleaned_count}.")

    if df.empty:
        print("Error: No valid data (with time and prob_zero > 0) was parsed. Cannot plot.")
        return
        
    # --- FIX 14: Time scaling removed ---
    
    print("\n--- Data Summary (Cleaned Data) ---")
    print(df.describe())
    print("-----------------------------------\n")

    # --- FIX 10: Print final plotted point count ---
    print(f"Plotting {len(df)} valid data points (particles).")
    # --- END FIX 10 ---
    
    # --- FIX 14: Use raw 'time' ---
    if df[['qubits', 'depth', 'time']].isnull().values.any():  
        print("Error: Data contains invalid (NaN) coordinates after cleaning. Cannot plot.")
        return
        
    min_qubits, max_qubits = df['qubits'].min(), df['qubits'].max()
    min_depth, max_depth = df['depth'].min(), df['depth'].max()
    max_time = df['time'].max() # <-- Use raw max_time
    # --- END FIX 14 ---

    print("Generating 3D interactive plot...")

    # --- FIX 14: Use raw 'time' ---
    points_xy = df[['qubits', 'depth']].values
    points_z_time = df['time'].values # <-- Use raw time
    points_c_prob = df['prob_zero'].values
    # --- END FIX 14 ---

    res_x, res_y = 500, 500  

    grid_x, grid_y = np.mgrid[min_qubits:max_qubits:(res_x+1)*1j, 
                             min_depth:max_depth:(res_y+1)*1j]

    print(f"Interpolating time values onto {res_x+1}x{res_y+1} grid...")
    grid_z = griddata(points_xy, points_z_time, (grid_x, grid_y), method='linear')  

    print(f"Interpolating probability values onto {res_x+1}x{res_y+1} grid...")
    grid_c = griddata(points_xy, points_c_prob, (grid_x, grid_y), method='linear')

    grid_z[np.isnan(grid_z)] = 0.0
    
    print("Creating vedo Grid and warping...")

    # Create the solid surface mesh
    grid_mesh = Grid(
        pos=((min_qubits + max_qubits) / 2, (min_depth + max_depth) / 2, 0),
        s=((max_qubits - min_qubits), (max_depth - min_depth)),
        res=(res_x, res_y)
    )
    
    warped_points = grid_mesh.points
    warped_points[:, 2] = grid_z.ravel()
    grid_mesh.points = warped_points


    # --- CUSTOM COLORMAP FIX (FIX 12 & 23) ---
    log_grid_c = np.log(grid_c.ravel())
    min_log_val = np.log(1e-9) 
    log_grid_c[np.isneginf(log_grid_c)] = min_log_val
    
    grid_mesh.pointdata["log_prob_zero"] = log_grid_c
    
    # 1. Apply colormap
    try:
        grid_mesh.cmap("viridis", "log_prob_zero")
    except Exception as e:
         print(f"CRITICAL: Could not apply colormap: {e}")

    # 2. Set transparency for the solid surface
    interpolated_surface_alpha = 0.9 
    grid_mesh.alpha(interpolated_surface_alpha) 

    # 3. Add scalar bar
    grid_mesh.add_scalarbar(title="Log(Probability of '0' State)", pos=((0.85, 0.1), (0.9, 0.9)), c='white')
    # --- END CUSTOM COLORMAP FIX ---

    # --- FIX 21: Ensure mesh renders as a surface ---
    grid_mesh.wireframe(False) # Explicitly turn off wireframe mode for the solid surface
    grid_mesh.lw(0)            # Set line width of mesh edges to 0
    # --- END FIX 21 ---
    
    # --- FIX 25: Clone *after* colormapping ---
    wireframe_mesh = grid_mesh.clone() # Clone the *colored* mesh
    
    # Add a tiny Z-offset to prevent z-fighting
    z_wire_offset = max_time * 0.002 # Increased offset
    wire_points = wireframe_mesh.points
    wire_points[:, 2] += z_wire_offset # Add the offset
    wireframe_mesh.points = wire_points
    
    wireframe_mesh.wireframe(True)     # Turn on wireframe mode
    # --- REMOVED .lc('white') to inherit colormap ---
    wireframe_mesh.alpha(0.5)          # Set wireframe alpha
    wireframe_mesh.lw(1.5)             # Set line width
    wireframe_mesh.name = "Wireframe Mesh"
    # --- END FIX 25 ---

    plt = Plotter(
        title="OTOC Sweep: Time vs. Qubits and Depth (Gridded)",  
        bg='black',
        axes={
            'xtitle': 'Number of Qubits', 
            'ytitle': 'Depth',
            'ztitle': 'Execution Time (s)',
            'c': 'white',
            'zrange': (0, max_time)  
        }
    )
    
    # --- Add raw data points for context (FIX 8, 13, 14) ---
    print("Adding raw data points to plot...")
    z_offset = max_time * 0.005 
    raw_coords = df[['qubits', 'depth', 'time']].values 
    raw_coords[:, 2] += z_offset 

    raw_points_mesh = Points(raw_coords, r=3, c='white', alpha=0.7) 
    raw_points_mesh.name = "Raw Data Points"
    
    plt.add(raw_points_mesh)
    # --- End raw data points ---

    plt.add(grid_mesh)      # Add the solid, semi-transparent grid surface
    plt.add(wireframe_mesh) # Add the wireframe on top of it

    print("Displaying plot in separate window...")
    print("NOTE: The screenshot will be saved *after* you close this window.")
    
    plt.show(zoom=1.0)
    
    try:
        print(f"Attempting to save screenshot to: {OUTPUT_FILE}...")
        plt.screenshot(OUTPUT_FILE, scale=20)  
        print(f"Successfully saved high-resolution screenshot to: {OUTPUT_FILE}")
    except Exception as e:
        print(f"Could not save screenshot: {e}")

    print("Done.")


if __name__ == "__main__":
    main()
