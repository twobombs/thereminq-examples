#!/usr/bin/env python3
# graph for the output of otoc_validationisingonly.sh
# generated by gemini25
# ... (all previous fixes) ...
# HANG FIX 5 (REVERT): Reverted from 'executor.submit' list
#          back to 'executor.map', which is lazy and won't hang.
#          Added progress dots to the 'executor.map' loop.

import re
import ast
import pandas as pd
from vedo import Plotter, settings
from vedo import Grid # Import Grid
import numpy as np
from scipy.interpolate import griddata
from pathlib import Path
import concurrent.futures
import os

LOG_DIR = "otoc_sweep_log"
OUTPUT_FILE = "otoc_sweep_3d_plot.png"

# --- HANG FIXES ---
DICT_SEARCH_LINE_LIMIT = 20000   # Max lines to search *inside* a dict
GLOBAL_SEARCH_LINE_LIMIT = 100000 # Max lines to read from one file
# --- END HANG FIXES ---

def parse_log_file(file_path):
    match = re.search(r"q(\d+)_d(\d+)\.log", file_path.name)
    if not match:
        return None
    
    # --- Return flags for all possible errors ---
    result = {
        "qubits": int(match.group(1)),
        "depth": int(match.group(2)),
        "time": None,
        "prob_zero": 0.0,
        "key_0_missing": False,
        "header_missing": False,
        "time_missing": False,
        "time_parse_error": False,
        "time_non_positive": False,
        "eof_in_dict": False,
        "global_line_limit_exceeded": False
    }
    # --- End flags ---

    time_match = None
    prob_zero_val = None
    found_header = False 
    in_dict = False      
    brace_level = 0
    prob_regex = re.compile(r"\b0:\s*([\d\.eE+-]+)")
    
    dict_line_counter = 0
    total_line_counter = 0

    try:
        with file_path.open('r') as f:
            for line in f:
                # --- HANG FIX 2: Global line limit ---
                total_line_counter += 1
                if total_line_counter > GLOBAL_SEARCH_LINE_LIMIT:
                    result["global_line_limit_exceeded"] = True
                    # This is a critical error, log it from the worker
                    print(f"\n--- PARSER WARNING: File {file_path.name} exceeded GLOBAL search limit {GLOBAL_SEARCH_LINE_LIMIT}. Skipping. ---")
                    return result # Give up on this file, but report the error
                # --- END HANG FIX 2 ---

                time_search = re.search(r"real\s+(\d+)m([\d\.]+)s", line)
                if time_search:
                    time_match = time_search
                
                if not found_header:
                    if "PyQrackIsing Probabilities:" in line:
                        found_header = True
                        start_match = re.search(r"(\{)", line)
                        if start_match:
                            in_dict = True
                            brace_level += line.count('{') - line.count('}')
                            if prob_zero_val is None:
                                prob_match = prob_regex.search(line)
                                if prob_match:
                                    prob_zero_val = float(prob_match.group(1))
                
                elif found_header and not in_dict:
                    start_match = re.search(r"(\{)", line)
                    if start_match:
                        in_dict = True
                        brace_level += line.count('{') - line.count('}')
                        if prob_zero_val is None:
                            prob_match = prob_regex.search(line)
                            if prob_match:
                                prob_zero_val = float(prob_match.group(1))
                
                elif in_dict:
                    # --- HANG FIX 1: Dict line limit ---
                    dict_line_counter += 1
                    if dict_line_counter > DICT_SEARCH_LINE_LIMIT:
                        print(f"\n--- PARSER WARNING: File {file_path.name} exceeded DICT search limit {DICT_SEARCH_LINE_LIMIT}. Skipping dict. ---")
                        in_dict = False
                        brace_level = 0
                        continue
                    # --- END HANG FIX 1 ---
                
                    if prob_zero_val is None:
                        prob_match = prob_regex.search(line)
                        if prob_match:
                            prob_zero_val = float(prob_match.group(1))
                    brace_level += line.count('{') - line.count('}')
                    if brace_level == 0:
                        in_dict = False
                        
        if in_dict:
            result["eof_in_dict"] = True

        # --- Parse Time ---
        if not time_match:
            result["time_missing"] = True
            return result # Still return data, but time will be None
        try:
            minutes = float(time_match.group(1))
            seconds = float(time_match.group(2))
            total_seconds = (minutes * 60) + seconds
            if total_seconds <= 0:
                result["time_non_positive"] = True
                return result # Still return, time will be None
            result["time"] = total_seconds
        except (ValueError, TypeError) as e:
            result["time_parse_error"] = True
            return result # Still return, time will be None

        # --- Parse Probs ---
        if not found_header:
            result["header_missing"] = True
        elif prob_zero_val is None:
            result["key_0_missing"] = True
        else:
            result["prob_zero"] = prob_zero_val

        return result
        
    except Exception as e:
        # This will be caught in the main loop
        return {"parse_exception": True, "file_name": file_path.name, "error": str(e)}


def main():
    log_path = Path(LOG_DIR)
    if not log_path.is_dir():
        print(f"Error: Log directory '{LOG_DIR}' not found.")
        return

    print(f"Scanning {LOG_DIR} for log files...")
    log_files = list(log_path.glob("q*_d*.log"))
    if not log_files:
        print(f"Error: No log files found in '{LOG_DIR}'.")
        return

    num_workers = os.cpu_count()
    print(f"Parsing {len(log_files)} log files using up to {num_workers} processes...")
    
    results_list = []
    
    # --- Counters for all warnings ---
    warnings = {
        "key_0_missing": 0,
        "header_missing": 0,
        "time_missing": 0,
        "time_parse_error": 0,
        "time_non_positive": 0,
        "eof_in_dict": 0,
        "global_line_limit_exceeded": 0
    }
    parse_exceptions = 0

    # --- HANG FIX: Use executor.map for lazy iteration ---
    with concurrent.futures.ProcessPoolExecutor(max_workers=num_workers) as executor:
        # executor.map is lazy and won't hang here
        results_iterator = executor.map(parse_log_file, log_files)
        
        print(f"Parsing {len(log_files)} files. Printing dot every 100 files:")
        processed_count = 0
        
        # Iterate over the results as they come in
        for result in results_iterator:
            processed_count += 1
            if processed_count % 100 == 0:
                print(".", end="", flush=True) # Print a dot
            
            if result is not None:
                if result.get("parse_exception", False):
                    parse_exceptions += 1
                    # print(f"\nPARSE ERROR on {result.get('file_name', 'N/A')}: {result.get('error', 'N/A')}")
                    continue
                
                results_list.append(result)
                for key in warnings:
                    if result.get(key, False):
                        warnings[key] += 1
    # --- END HANG FIX ---
        
    print(f"\nParsing complete.") # Add a newline after the dots
        
    print(f"\nSuccessfully parsed {len(results_list)} log files.")
    
    # --- Print all warning summaries ---
    if warnings["header_missing"] > 0:
        print(f"INFO: {warnings['header_missing']} log files were missing the 'PyQrackIsing Probabilities' header.")
    if warnings["key_0_missing"] > 0:
        print(f"INFO: {warnings['key_0_missing']} log files had the header but were missing the '0' key.")
    if warnings["time_missing"] > 0:
        print(f"WARNING: {warnings['time_missing']} log files were missing the 'real time' string.")
    if warnings["time_parse_error"] > 0:
        print(f"WARNING: {warnings['time_parse_error']} log files had an unparseable 'real time' string.")
    if warnings["time_non_positive"] > 0:
        print(f"WARNING: {warnings['time_non_positive']} log files had a non-positive 'real time'.")
    if warnings["eof_in_dict"] > 0:
        print(f"WARNING: {warnings['eof_in_dict']} log files ended unexpectedly while inside a dict.")
    if warnings["global_line_limit_exceeded"] > 0:
        print(f"WARNING: {warnings['global_line_limit_exceeded']} log files exceeded the global line limit and were skipped.")
    if parse_exceptions > 0:
        print(f"WARNING: {parse_exceptions} files caused a critical parse exception.")
    
    df = pd.DataFrame(results_list)

    # --- Data Cleaning ---
    original_count = len(df)
    df = df.dropna(subset=['time'])
    cleaned_count = len(df)
    if original_count != cleaned_count:
        print(f"INFO: Dropped {original_count - cleaned_count} rows due to missing/invalid time data.")

    if df.empty:
        print("Error: No valid data (with time) was parsed. Cannot plot.")
        return
        
    df['scaled_time'] = df['time'] * 10.0
    
    print("\n--- Data Summary (Cleaned Data) ---")
    print(df.describe())
    print("-----------------------------------\n")
    
    if df[['qubits', 'depth', 'scaled_time']].isnull().values.any(): 
        print("Error: Data contains invalid (NaN) coordinates after cleaning. Cannot plot.")
        return
        
    min_qubits, max_qubits = df['qubits'].min(), df['qubits'].max()
    min_depth, max_depth = df['depth'].min(), df['depth'].max()
    max_scaled_time = df['scaled_time'].max() 

    print("Generating 3D interactive plot...")

    points_xy = df[['qubits', 'depth']].values
    points_z_time = df['scaled_time'].values
    points_c_prob = df['prob_zero'].values

    res_x, res_y = 500, 500 

    grid_x, grid_y = np.mgrid[min_qubits:max_qubits:(res_x+1)*1j, 
                              min_depth:max_depth:(res_y+1)*1j]

    print(f"Interpolating time values onto {res_x+1}x{res_y+1} grid...")
    grid_z = griddata(points_xy, points_z_time, (grid_x, grid_y), method='linear') 

    print(f"Interpolating probability values onto {res_x+1}x{res_y+1} grid...")
    grid_c = griddata(points_xy, points_c_prob, (grid_x, grid_y), method='linear')

    grid_z[np.isnan(grid_z)] = 0.0
    grid_c[np.isnan(grid_c)] = 0.0
    
    print("Creating vedo Grid and warping...")

    grid_mesh = Grid(
        pos=((min_qubits + max_qubits) / 2, (min_depth + max_depth) / 2, 0),
        s=((max_qubits - min_qubits), (max_depth - min_depth)),
        res=(res_x, res_y)
    )
    
    warped_points = grid_mesh.points
    warped_points[:, 2] = grid_z.ravel()
    grid_mesh.points = warped_points

    # --- CUSTOM COLORMAP FIX ---
    log_grid_c = np.log1p(grid_c.ravel())
    grid_mesh.pointdata["log_prob_zero"] = log_grid_c
    
    # 1. Apply the standard 'viridis' colormap to all points
    grid_mesh.cmap("viridis", "log_prob_zero")
    
    # 2. Manually select points where log_prob_zero is 0 and set them to black
    try:
        # This function selects points based on the scalar value
        grid_mesh.select_points_by_scalar("log_prob_zero", 0.0, 1e-9).color('black')
    except Exception as e:
        print(f"\nWarning: Could not apply black color override: {e}")
        print("This may be due to an old 'vedo' version. Continuing with standard colormap.")
    
    grid_mesh.alpha(1.0) 
    grid_mesh.add_scalarbar(title="Log(1 + Probability of '0' State)", pos=((0.85, 0.1), (0.9, 0.9)), c='white')
    # --- END CUSTOM COLORMAP FIX ---


    plt = Plotter(
        title="OTOC Sweep: Time vs. Qubits and Depth (Gridded)",  
        bg='black',
        axes={
            'xtitle': 'Number of Qubits',
            'ytitle': 'Depth',
            'ztitle': 'Scaled Execution Time (s) [10x]',
            'c': 'white',
            'zrange': (0, max_scaled_time) 
        }
    )
    
    plt.add(grid_mesh)

    print("Displaying plot in separate window...")
    print("NOTE: The screenshot will be saved *after* you close this window.")
    
    plt.show(zoom=1.0)
    
    try:
        print(f"Attempting to save screenshot to: {OUTPUT_FILE}...")
        plt.screenshot(OUTPUT_FILE) # <-- Fixed typo
        print(f"Successfully saved screenshot to: {OUTPUT_FILE}")
    except Exception as e:
        print(f"Could not save screenshot: {e}")

    print("Done.")


if __name__ == "__main__":
    main()
